{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "723d9fc3-a52f-49ca-b93f-1931e37dd310",
   "metadata": {},
   "source": [
    "# Import Packages und Functions\n",
    "\n",
    "## Code: Breitstellen der Pakete und Funktionen\n",
    "Führen Sie den unten stehnden Code aus um alle notwendigen Zusatzpakete und Funktionen verfügbar zu machen. Dieser Abschnitt muss nur ein einziges mal ausgeführt werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55440c4-20d4-4048-80fe-619d89417a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # used to handle csv- or excel files as a dataframe (table object)\n",
    "import numpy as np # used for basic mathematical operations\n",
    "import matplotlib.pyplot as plt # package for basic data plotting\n",
    "import seaborn as sns # additional package with more plotting options based on pyplot\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
    "\n",
    "# from here code for more complexe plotting functions\n",
    "def _resolve_column(df: pd.DataFrame, col):\n",
    "    \"\"\"Resolve a column specifier for flat or MultiIndex columns.\n",
    "    - col can be None, a tuple (MultiIndex), or a string.\n",
    "    - Exact matches are preferred. If not found and df has MultiIndex,\n",
    "      try to find columns where any level equals the string.\n",
    "    \"\"\"\n",
    "    if col is None:\n",
    "        return None, None  # (series, display_name)\n",
    "    # tuple (explicit MultiIndex)\n",
    "    if isinstance(col, tuple):\n",
    "        if col in df.columns:\n",
    "            return df[col], \" - \".join(map(str, col))\n",
    "        raise KeyError(f\"Column tuple {col} not found in DataFrame columns.\")\n",
    "    # string case\n",
    "    # direct exact match (flat columns)\n",
    "    if col in df.columns:\n",
    "        return df[col], str(col)\n",
    "    # if MultiIndex, try exact match on any level\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        # exact level match\n",
    "        matches = [c for c in df.columns if any(str(level) == col for level in c)]\n",
    "        if len(matches) == 1:\n",
    "            return df[matches[0]], \" - \".join(map(str, matches[0]))\n",
    "        if len(matches) > 1:\n",
    "            # prefer a match where top-level equals col\n",
    "            top_matches = [c for c in matches if str(c[0]) == col]\n",
    "            chosen = top_matches[0] if top_matches else matches[0]\n",
    "            print(f\"Multiple columns match '{col}', using {chosen}.\")\n",
    "            return df[chosen], \" - \".join(map(str, chosen))\n",
    "        # try substring match (e.g., user passes 'PC 1' and column is ('PCA','PC 1'))\n",
    "        substr_matches = [c for c in df.columns if any(col in str(level) for level in c)]\n",
    "        if len(substr_matches) >= 1:\n",
    "            chosen = substr_matches[0]\n",
    "            print(f\"No exact match for '{col}', using first substring match {chosen}.\")\n",
    "            return df[chosen], \" - \".join(map(str, chosen))\n",
    "    # fallback: try substring in flat columns\n",
    "    flat_sub = [c for c in df.columns if col in str(c)]\n",
    "    if len(flat_sub) >= 1:\n",
    "        chosen = flat_sub[0]\n",
    "        print(f\"No exact match for '{col}', using first flat substring match {chosen}.\")\n",
    "        return df[chosen], str(chosen)\n",
    "    raise KeyError(f\"Column '{col}' not found in DataFrame columns.\")\n",
    "\n",
    "def plot_dataframe(df: pd.DataFrame,\n",
    "                   x,\n",
    "                   y,\n",
    "                   z=None,\n",
    "                   hue=None,\n",
    "                   figsize=(9, 6),\n",
    "                   palette=None,\n",
    "                   point_size=40,\n",
    "                   alpha=0.9,\n",
    "                   cmap=\"viridis\",\n",
    "                   title=None):\n",
    "    \"\"\"\n",
    "    Flexible plotting for DataFrame: automatic 2D/3D scatter depending on z.\n",
    "    x, y, z, hue can be strings or tuples (for MultiIndex). If z is None -> 2D.\n",
    "    Returns (fig, ax).\n",
    "    \"\"\"\n",
    "    # Resolve columns and display names\n",
    "    X, x_label = _resolve_column(df, x)\n",
    "    Y, y_label = _resolve_column(df, y)\n",
    "    Z, z_label = _resolve_column(df, z) if z is not None else (None, None)\n",
    "    H, h_label = _resolve_column(df, hue) if hue is not None else (None, None)\n",
    "\n",
    "    if X is None or Y is None:\n",
    "        raise ValueError(\"x and y must be provided and resolvable to DataFrame columns.\")\n",
    "\n",
    "    x_vals = X.values\n",
    "    y_vals = Y.values\n",
    "    z_vals = Z.values if Z is not None else None\n",
    "    hue_vals = H.values if H is not None else None\n",
    "\n",
    "    is_3d = z_vals is not None\n",
    "\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    if is_3d:\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "    else:\n",
    "        ax = fig.add_subplot(111)\n",
    "\n",
    "    # No hue: single color\n",
    "    if hue_vals is None:\n",
    "        color = sns.color_palette()[0]\n",
    "        if is_3d:\n",
    "            sc = ax.scatter(x_vals, y_vals, z_vals, s=point_size, alpha=alpha, color=color)\n",
    "        else:\n",
    "            sc = ax.scatter(x_vals, y_vals, s=point_size, alpha=alpha, color=color)\n",
    "    else:\n",
    "        # Determine whether hue is numeric or categorical.\n",
    "        # If dtype is object or string-like -> categorical.\n",
    "        if pd.api.types.is_numeric_dtype(H):\n",
    "            # continuous hue\n",
    "            if is_3d:\n",
    "                # 3D scatter with continuous color: map to RGBA\n",
    "                norm = plt.Normalize(np.nanmin(hue_vals), np.nanmax(hue_vals))\n",
    "                cmap_obj = plt.get_cmap(cmap)\n",
    "                colors = cmap_obj(norm(hue_vals))\n",
    "                sc = ax.scatter(x_vals, y_vals, z_vals, c=colors, s=point_size, alpha=alpha)\n",
    "                # create a ScalarMappable for colorbar\n",
    "                mappable = plt.cm.ScalarMappable(norm=norm, cmap=cmap_obj)\n",
    "                mappable.set_array(hue_vals)\n",
    "                cbar = fig.colorbar(mappable, ax=ax, pad=0.1)\n",
    "                cbar.set_label(h_label or str(hue))\n",
    "            else:\n",
    "                sc = ax.scatter(x_vals, y_vals, c=hue_vals, cmap=cmap, s=point_size, alpha=alpha)\n",
    "                cbar = fig.colorbar(sc, ax=ax, pad=0.1)\n",
    "                cbar.set_label(h_label or str(hue))\n",
    "        else:\n",
    "            # categorical hue (strings or objects)\n",
    "            categories, uniques = pd.factorize(hue_vals)\n",
    "            n_cats = len(uniques)\n",
    "            if palette is None:\n",
    "                palette = sns.color_palette(n_colors=n_cats)\n",
    "            colors = [palette[i % len(palette)] for i in categories]\n",
    "            if is_3d:\n",
    "                sc = ax.scatter(x_vals, y_vals, z_vals, c=colors, s=point_size, alpha=alpha)\n",
    "            else:\n",
    "                sc = ax.scatter(x_vals, y_vals, c=colors, s=point_size, alpha=alpha)\n",
    "            # legend\n",
    "            handles = []\n",
    "            for i, lab in enumerate(uniques):\n",
    "                handles.append(plt.Line2D([], [], marker='o', color=palette[i % len(palette)], linestyle='', markersize=6))\n",
    "            ax.legend(handles, uniques, title=(h_label or str(hue)), bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    # Labels and title\n",
    "    ax.set_xlabel(x_label or str(x))\n",
    "    ax.set_ylabel(y_label or str(y))\n",
    "    if is_3d:\n",
    "        ax.set_zlabel(z_label or str(z))\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d59355-a048-42cd-b426-ade3de430e0b",
   "metadata": {},
   "source": [
    "# Daten Import\n",
    "\n",
    "Im 1. Schritt müssen die Daten importiert werden. Hierfür wird das Datenbankpacket \"pandas\" verwendet. Pandas kann eine Vielzahl unterschiedliche Datentypen wie csv, excel, oder aber auch png lesen. Native Formate, die kein zusätzliches Programm benötigen, wie zum Beispiel \"csv\" sind für große Datensätze zu bevorzugen, da der Umweg über Excel unter Umständen sehr langsams sein kann.\n",
    "\n",
    "## Code: Daten von csv einladen\n",
    "\n",
    "Im Ordner \"Data/CSV/\" liegen zwei Datensätze vor. \n",
    "\n",
    "OriginalData.csv enhält grundlegende Auswertungen (Härte, E-Modul, Steifigkeit) zu den einzelnen Indents.\n",
    "\n",
    "DataWithAdditional_Infos.csv enthält die selben Daten und noch zusätzliche Informationen über die mathemathische Beschreibung der Belastungs- und Endlastungskurve, so wie der Verläufe von Härte und Modulus über die Eindringtiefe. Diese Daten wurden durch fitting der Rohdaten gewonnen und können einen Datensatz um wertvolle Informationen erweitern. Zum Beginn können jedoch die \"OriginalData.csv\" eingeladen werden, um eine besser Übersichtlichkeit über die Datenstruktur zu erhalten.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269ac4b0-a68c-4745-aed5-68fd3c04351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=== change parameter from here ===#\n",
    "\n",
    "file_path:str = \"Data/CSV/DataWithAdditional_Infos.csv\" #<-- hier muss der relative Dateipfad eingefügt werden. Beispiel \"Data/CSV/MyData.csv\" \n",
    "\n",
    "#=== dont change paramter from here ===#\n",
    "\n",
    "# creating data frame from nanoindendation data\n",
    "df = pd.read_csv(file_path, # path pointing to the file to be importe d \n",
    "                header = [0,1], # used because the dataframe has multi-index column names \"(\"MODULUS GPa\",\"mean\") & (\"MODULLUS GPa\", \"std\")\" as example\n",
    "                sep = \";\", # defines the separator used in csv file to separate different columns \n",
    "                decimal = \",\" #telling pandas the instad of \".\" a comma \",\" is used as decimal point (german number format)\n",
    "                )\n",
    "# Set Indent column as Index\n",
    "df.index = df[(\"Unnamed: 0_level_0\",\"Unnamed: 0_level_1\")]\n",
    "df = df.drop((\"Unnamed: 0_level_0\",\"Unnamed: 0_level_1\"), axis=1)\n",
    "\n",
    "#visualization of the data structure\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d2c678-103a-4da3-a59c-aed62245b113",
   "metadata": {},
   "source": [
    "# Daten Visualisierung\n",
    "In den eingelesenen Daten sind jedem Indent jeweils zwei x- und y-Koordinaten zugeordnet: **„absolut“** und **„real“**. Der vor der Messung festgelegte Abstand zwischen den Indents beträgt 4 µm. Das nachträglich durchgeführte Mapping mit dem Rasterkraftmikroskop zeigt jedoch, dass dieser Abstand – insbesondere bei kleinen Schrittweiten – **deutlich von der eingestellten Schrittweite abweichen kann**.\n",
    "\n",
    "Die **„absoluten“ Koordinaten** entsprechen den idealisierten Positionen aus den Nanoindentierungsdaten, basierend auf der geplanten Schrittweite. Die **„realen“ Koordinaten** hingegen wurden mithilfe der Rasterkraftmikroskopie (AFM) nachträglich bestimmt und spiegeln die tatsächliche Position der Indents wider.\n",
    "\n",
    "Wenn man mehrere Datensätze überlagern möchte – wie in diesem Fall die Nanoindentierungsdaten mit den AFM-Daten (oder auch mit anderen Methoden wie z. B. EBSD) – ist es notwendig, die **realen Indentpositionen nachträglich in den Datensatz zu integrieren**. Durch Bildverarbeitung und eine geeignete Skalierung (z. B. auf eine Bildgröße von 50 µm × 50 µm) können die Indentzentren bestimmt werden.\n",
    "\n",
    "Im vorliegenden Beispiel wurde dieser Schritt bereits mit der Software **ImageJ** durchgeführt, um Zeit zu sparen.\n",
    "## Code: Erstellen eins Härte- und E-Modulu Mappings\n",
    "Erstellen Sie mit dem unten liegenden Code für die Härte, das E-Modul jeweils ein Mapping. Passen Sie hier für gegebenenfalls die im Code hervorgehobenen Parameter an.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98a67d3-633b-4887-88bc-98d90201d083",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#=== change parameter from here ===#\n",
    "#marker options\n",
    "marker_shape = \"s\" #<-- \"s\" for squares, \">\" for triangles\n",
    "marker_size = 85 # setting the size of the markers\n",
    "marker_tranparency = 0.75 # setting the transparency of the markers\n",
    "\n",
    "#choosing between ideal indent position and real indent positions\n",
    "indent_position = \"real\" # <-- \"real\" or \"ideal\"\n",
    "\n",
    "#change the colormap of the plots\n",
    "colormap = \"crest_r\" #<-- \"viridis\", \"cividis\", \"inferno\" ,\"magma\", \"plasma\", \"rocket\", \"flare\", \"crest\", \"copper\"\n",
    "#=========================#\n",
    "\n",
    "#=== dont change paramter from here ===#\n",
    "#image size\n",
    "image_width_cm = 20 #change value to alter the image size\n",
    "image_height_cm = 20 # change value to alter the image size\n",
    "\n",
    "#Atomic Force Microscopy Mapping of Indentationmapping\n",
    "background_image = plt.imread(\"Data/Images/BackGround.png\")\n",
    "dx,dy = -2.5,-3.2 #parameter for adjusting the background image\n",
    "range_um = 50 # parameter for adjusting the background image\n",
    "\n",
    "# automatical column selection for indent position\n",
    "if indent_position == \"real\":\n",
    "    x = (\"x\",\"real\") # defining x axis\n",
    "    y = (\"y\",\"real\") # defining y axis\n",
    "elif indent_position ==\"ideal\":\n",
    "    x = (\"x\",\"absolut\") # defining x axis\n",
    "    y = (\"y\",\"absolut\") # defining x axis\n",
    "\n",
    "#creating a figure object\n",
    "fig, ax = plt.subplots(nrows = 1,\n",
    "                       ncols = 3, #3 images next to each other\n",
    "                       sharey=True)\n",
    "\n",
    "\n",
    "fig.set_dpi(600) # increasing the resolution of the plot\n",
    "fig.set_size_inches(image_width_cm/2.5,image_height_cm/2.54) #calcuating image size\n",
    "\n",
    "# hardness plot \n",
    "sns.scatterplot(data = df,\n",
    "                x = x,\n",
    "                y = y,\n",
    "                hue = (\"HARDNESS GPa\",\"mean\"),\n",
    "                ax = ax[0], #left plot\n",
    "                marker = marker_shape, #square marker\n",
    "                palette = sns.color_palette(colormap,as_cmap = True), # defining the color plaette\n",
    "                s = marker_size, # marker size\n",
    "                edgecolor = None, # remove the outline of the markers\n",
    "                alpha = marker_tranparency , # transparence of the markers infill\n",
    "               )\n",
    "\n",
    "if indent_position == \"real\":\n",
    "    ax[0].imshow(background_image,\n",
    "                 extent=[dx,range_um+dx,dy,range_um+dy],\n",
    "                 aspect='equal',\n",
    "                 zorder=-1,\n",
    "                 origin = \"upper\"\n",
    "                )\n",
    "\n",
    "#modulus plot\n",
    "sns.scatterplot(data = df,\n",
    "                x = x,\n",
    "                y = y,\n",
    "                hue = (\"MODULUS GPa\",\"mean\"),\n",
    "                ax = ax[1], #middle plot\n",
    "                marker = marker_shape, #square marker\n",
    "                palette = sns.color_palette(colormap,as_cmap = True), # defining the color plaette\n",
    "                s = marker_size, # marker size\n",
    "                edgecolor = None, # remove the outline of the markers\n",
    "                alpha = marker_tranparency , # transparence of the markers infill\n",
    "                )\n",
    "\n",
    "if indent_position == \"real\":\n",
    "    ax[1].imshow(background_image,\n",
    "                 extent=[dx,range_um+dx,dy,range_um+dy],\n",
    "                 aspect='equal',\n",
    "                 zorder=-1,\n",
    "                 origin = \"upper\"\n",
    "                )\n",
    "#stiffnesplot \n",
    "sns.scatterplot(data = df,\n",
    "                x = x,\n",
    "                y = y,\n",
    "                hue = (\"S2overP\",\"mean\"), \n",
    "                ax = ax[2], #right plot\n",
    "                marker = marker_shape, #square marker\n",
    "                palette = sns.color_palette(colormap,as_cmap = True), # defining the color plaette\n",
    "                s = marker_size, # marker size\n",
    "                edgecolor = None, # remove the outline of the markers\n",
    "                alpha = marker_tranparency , # transparence of the markers infill\n",
    "                )\n",
    "\n",
    "if indent_position == \"real\":\n",
    "    ax[2].imshow(background_image,\n",
    "                 extent=[dx,range_um+dx,dy,range_um+dy],\n",
    "                 aspect='equal',\n",
    "                 zorder=-1,\n",
    "                 origin = \"upper\"\n",
    "                )\n",
    "\n",
    "for item in ax:\n",
    "    item.set_aspect(\"equal\") # ensure äquidistance on x and y axis\n",
    "    sns.move_legend(loc = \"lower center\", bbox_to_anchor = (0.5,1), obj = item) # move legend above the corresponding plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57756af-c866-4a79-bbdf-79eb4e5804c1",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "## Die richtigen Daten auswählen\n",
    "\n",
    "Mit dem Parameter `n_components` bestimmst du, auf wie viele Hauptkomponenten dein Datensatz reduziert wird. Häufig werden 2 oder 3 Komponenten gewählt, da sich diese gut grafisch darstellen lassen.\n",
    "\n",
    "Die PCA hilft dabei, Datensätze mit vielen Merkmalen (z. B. 5 Features = 5 Dimensionen) auf weniger Dimensionen zu reduzieren. Dabei bleibt die grundlegende Struktur der Daten erhalten – also wie die Messpunkte zueinander in Beziehung stehen. Ziel ist es, die wichtigsten Muster in den Daten sichtbar zu machen und gleichzeitig unwichtige oder redundante Informationen zu reduzieren.\n",
    "\n",
    "## Wie man PCA-Ergebnisse interpretiert\n",
    "\n",
    "Die Principal Component Analysis (PCA) hilft dabei, komplexe Datensätze mit vielen Merkmalen auf wenige Hauptkomponenten zu reduzieren. Dadurch lassen sich Muster und Strukturen in den Daten leichter erkennen und visualisieren.\n",
    "\n",
    "Ein zentraler Anhaltspunkt ist der **Anteil der erklärten Varianz**. Er zeigt, wie viel Information jede Hauptkomponente aus den ursprünglichen Daten aufnimmt. Wenn zum Beispiel die ersten beiden Komponenten zusammen 85 % der Varianz erklären, bedeutet das, dass sie den Großteil der Datenstruktur abbilden. In diesem Fall reicht es oft aus, nur diese beiden Komponenten für die weitere Analyse oder Visualisierung zu verwenden.\n",
    "\n",
    "Im **PCA-Plot** wird jeder Datenpunkt (z. B. ein Indent) anhand seiner Position im Raum der Hauptkomponenten dargestellt. Liegen Punkte nah beieinander, sind sie sich in ihren Eigenschaften ähnlich. Gruppen oder Cluster im Plot deuten auf vergleichbares Materialverhalten hin, während Ausreißer auf besondere oder fehlerhafte Messungen hinweisen können.\n",
    "\n",
    "Die sogenannten **Loadings** geben Auskunft darüber, wie stark jedes ursprüngliche Merkmal zu den Hauptkomponenten beiträgt. Sie helfen dabei zu verstehen, was eine Komponente physikalisch bedeutet. Wenn zum Beispiel die erste Komponente stark von der Härte und dem Elastizitätsmodul beeinflusst wird, beschreibt sie vermutlich die allgemeine Steifigkeit des Materials. Eine andere Komponente könnte vor allem durch die Streuung der Messwerte geprägt sein und damit auf die Homogenität oder Heterogenität des Materials hinweisen.\n",
    "\n",
    "Durch die Kombination dieser Informationen kannst du besser einschätzen, **welche Merkmale die Struktur deiner Daten dominieren** – und wie du sie gezielt für das Clustering oder die Interpretation nutzen kannst.\n",
    "\n",
    "## Warum müssen die Daten skaliert werden?\n",
    "\n",
    "Bevor PCA oder KMeans angewendet werden, ist es wichtig, die Daten zu **skalieren** – also alle Merkmale (Features) auf einen vergleichbaren Wertebereich zu bringen.\n",
    "\n",
    "Das liegt daran, dass viele Algorithmen (wie PCA und KMeans) auf **Abständen im Merkmalsraum** basieren. Wenn ein Feature (z. B. der Elastizitätsmodul in GPa) viel größere Zahlenwerte hat als ein anderes (z. B. die Härte in GPa), dann **dominiert dieses Feature die Analyse**, obwohl es nicht unbedingt wichtiger ist.\n",
    "\n",
    "Durch das **Standardisieren** (z. B. mit `StandardScaler`) wird jedes Feature so umgerechnet, dass es den **Mittelwert 0 und die Standardabweichung 1** hat. Dadurch tragen alle Merkmale **gleich stark zur Analyse bei** – unabhängig von ihrer ursprünglichen Einheit oder Größenordnung.\n",
    "\n",
    "---\n",
    "\n",
    "## Code: Durchführen der PCA  \n",
    "Probiere verschiedene Werte für `n_components` aus (z. B. 2 und 3) und erweitere den `features`-DataFrame um zusätzliche Messgrößen, die für das Clustering relevant sein könnten – z. B. die Standardabweichung von Härte, Elastizitätsmodul oder S²/P. Scrolle bei Bedarf nach oben, um dir die Struktur des ursprünglichen DataFrames noch einmal anzusehen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78355c84-0573-43a0-8a24-6accdbd1f6c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#=== change parameter ===#\n",
    "n_components = 2 #<-- number of components \n",
    "\n",
    "#selecting important columns for clustering\n",
    "features = df[[\n",
    "                (\"HARDNESS GPa\",\"mean\"),\n",
    "                (\"MODULUS GPa\",\"mean\"),\n",
    "                (\"S2overP\",\"mean\"),\n",
    "                (\"LoadUnloadAnalysis\",\"hf\"),\n",
    "                (\"LoadUnloadAnalysis\",\"Energy_Dissipated\")\n",
    "                ]].copy()\n",
    "#=========================#\n",
    "\n",
    "#=== dont change paramter from here ===#\n",
    "\n",
    "#deleting indents with not data (Indent 15, Indent 67) \n",
    "features.dropna(inplace = True)\n",
    "\n",
    "# scale data very important for correct results!\n",
    "scaler = StandardScaler() \n",
    "X_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# PCA on scaled data\n",
    "pca = PCA(n_components=n_components) \n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6)) \n",
    "if X_pca.shape[1] == 3:\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c='steelblue', s=40)\n",
    "    ax.set_xlabel('PC 1')\n",
    "    ax.set_ylabel('PC 2')\n",
    "    ax.set_zlabel('PC 3')\n",
    "    ax.set_title('PCA Scatterplot (3D)')\n",
    "    plt.show()\n",
    "    print(\"Erklärte Varianzanteile:\", pca.explained_variance_ratio_)\n",
    "    loadings = pd.DataFrame(pca.components_.T, columns=['PCA1', 'PCA2',\"PCA3\"], index=features.columns ) \n",
    "    print(loadings)\n",
    "\n",
    "else:\n",
    "    plt.scatter(X_pca[:, 0], X_pca[:, 1], c='steelblue', s=40)\n",
    "    plt.xlabel('PC 1')\n",
    "    plt.ylabel('PC 2')\n",
    "    plt.title('PCA Scatterplot (2D)')\n",
    "    plt.show()\n",
    "    print(\"Erklärte Varianzanteile:\", pca.explained_variance_ratio_)\n",
    "    loadings = pd.DataFrame(pca.components_.T, columns=['PC 1', 'PC 2'], index=features.columns ) \n",
    "    print(loadings)\n",
    "\n",
    "# deleting PCA columns in orignial data dataframe if present \n",
    "if 'PCA' in df.columns.get_level_values(0):\n",
    "    df = df.drop(columns='PCA', level=0)\n",
    "\n",
    "# make dataframe for PCA1 and PCA2 and maybe PCA3 axis\n",
    "pca_columns = [f'PC {i+1}' for i in range(X_pca.shape[1])]\n",
    "pca_df = pd.DataFrame(X_pca, columns=pca_columns, index=features.index)\n",
    "\n",
    "# transform to multi index column (\"PCA\",\"PC 1),(\"PC\",\"PC 2\"),...\n",
    "pca_df.columns = pd.MultiIndex.from_product([['PCA'], pca_columns])\n",
    "\n",
    "# join pca_df with orignal DataFrame df by Index \"Indent Nr\"\n",
    "df = df.join(pca_df)\n",
    "\n",
    "#disply orignal DataFrame with PCA columns\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de70102-0cb0-4afe-8622-0478d91fc7e0",
   "metadata": {},
   "source": [
    "# Clustering mit KMeans\n",
    "\n",
    "## Was ist KMeans und warum ist es für Nanoindentierung sinnvoll?\n",
    "\n",
    "KMeans ist ein sogenannter **Clustering-Algorithmus**. Er wird verwendet, um Datenpunkte automatisch in Gruppen (sogenannte *Cluster*) einzuteilen – und zwar **ohne dass man vorher wissen muss, welche Gruppen es gibt**. Das macht KMeans zu einem typischen Verfahren des *unüberwachten Lernens*.\n",
    "\n",
    "1. Es werden zufällig `k` Clusterzentren initialisiert.\n",
    "2. Jeder Datenpunkt wird dem nächstgelegenen Zentrum zugewiesen.\n",
    "3. Die Clusterzentren werden neu berechnet – als Mittelwert aller zugehörigen Punkte.\n",
    "4. Die Schritte 2 und 3 wiederholen sich, bis sich die Clusterzuweisungen nicht mehr ändern oder ein Abbruchkriterium erreicht ist.\n",
    "\n",
    "In der Nanoindentierung misst man an vielen Punkten mechanische Eigenschaften wie **Härte**, **Elastizitätsmodul** oder **S²/P**. Diese Werte können sich je nach Materialphase, Mikrostruktur oder Oberflächenzustand unterscheiden. KMeans hilft dabei, **ähnliche Indents zu gruppieren**, sodass man z. B. verschiedene Gefügezustände oder Schichten im Material erkennen kann – **rein datengetrieben**, ohne dass man vorher wissen muss, wo sich diese befinden.\n",
    "\n",
    "---\n",
    "\n",
    "## Vorbereitung: Wie viele Cluster sind sinnvoll?\n",
    "\n",
    "### Bewertung der Clusterqualität: Inertia & Silhouette Score\n",
    "\n",
    "Beim Clustering mit KMeans stellt sich oft die Frage: **Wie viele Cluster sind sinnvoll?** Da KMeans ein unüberwachtes Verfahren ist, gibt es keine „richtige“ Anzahl an Clustern – man muss sie aus den Daten herausfinden. Zwei wichtige Kennzahlen helfen dabei:\n",
    "\n",
    "#### Inertia (Trägheit)\n",
    "\n",
    "Die Inertia misst, **wie weit die Datenpunkte im Durchschnitt von ihrem jeweiligen Clusterzentrum entfernt sind**. Je kleiner die Inertia, desto kompakter sind die Cluster.\n",
    "\n",
    "- **Interpretation:**  \n",
    "  Eine niedrige Inertia bedeutet, dass die Datenpunkte gut zu ihren Clustern passen. Allerdings sinkt die Inertia immer, wenn man mehr Cluster hinzufügt – auch dann, wenn es keinen echten Mehrwert bringt. Deshalb reicht die Inertia allein nicht aus, um die optimale Clusteranzahl zu bestimmen.\n",
    "\n",
    "- **Elbow-Methode:**  \n",
    "  Trägt man die Inertia gegen die Anzahl der Cluster auf, entsteht oft eine Kurve mit einem „Knick“ (engl. *elbow*). Dieser Knick zeigt, **ab wann zusätzliche Cluster nur noch wenig Verbesserung bringen**. Der Punkt des Knicks ist ein guter Kandidat für die optimale Clusteranzahl.\n",
    "\n",
    "#### Silhouette Score\n",
    "\n",
    "Der Silhouette Score bewertet, **wie gut ein Punkt zu seinem eigenen Cluster passt – im Vergleich zu anderen Clustern**. Er liegt zwischen –1 und +1:\n",
    "\n",
    "- **Nahe +1:** Punkt ist gut im eigenen Cluster platziert und weit von anderen Clustern entfernt → gute Trennung  \n",
    "- **Nahe 0:** Punkt liegt zwischen zwei Clustern → keine klare Zuordnung  \n",
    "- **Negativ:** Punkt ist vermutlich im falschen Cluster → schlechte Struktur\n",
    "\n",
    "- **Interpretation:**  \n",
    "  Ein hoher Silhouette Score spricht für eine sinnvolle Clusterstruktur. Im Gegensatz zur Inertia kann der Score auch wieder sinken, wenn zu viele Cluster gewählt werden – das macht ihn besonders nützlich zur Bewertung.\n",
    "\n",
    "---\n",
    "\n",
    "## Code: Bestimmung der geeigneten Anazhl an Cluster\n",
    "\n",
    "Untersuchen Sie, **wie viele Cluster für die KMeans-Analyse am besten geeignet sind**. Nutzen Sie dazu die Visualisierung der Inertia und des Silhouette Scores.  \n",
    "Gehen Sie bei Bedarf einen Schritt zurück und **passen Sie die PCA oder das Feature-Set an**, um zu prüfen, wie sich die Scores verändern.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef21ed2-9ff3-4968-84c4-4b93f3db3c9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "\"\"\"\n",
    "Computes and plots Silhouette Score and Inertia (Elbow Plot) for different numbers of clusters.\n",
    "    \n",
    "Parameters:\n",
    "- df: DataFrame containing the data and possibly PCA columns\n",
    "- features_source: 'PCA' or 'features' → determines which data basis to use\n",
    "- cluster_range: range of cluster numbers to test (e.g., range(2, 10))\n",
    " \"\"\"\n",
    "\n",
    "#=== Change from here ===#\n",
    "features_source = \"PCA\" #<-- \"PCA\" or \"features\"\n",
    "max_number_cluster = 10\n",
    "#========================#\n",
    "\n",
    "#=== Dont change from here ===#\n",
    "silhouette_scores = []\n",
    "inertias = []\n",
    "ks = []\n",
    "\n",
    "# Select data basis\n",
    "if features_source == 'PCA':\n",
    "    X = df['PCA'].dropna()\n",
    "elif features_source == 'features':\n",
    "    X = features.copy()\n",
    "else:\n",
    "    raise ValueError(\"features_source must be 'PCA' or 'features'.\")\n",
    "\n",
    "for k in range(2,max_number_cluster):\n",
    "    kmeans = KMeans(n_clusters=k, n_init='auto', random_state=42)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    inertia = kmeans.inertia_\n",
    "    inertias.append(inertia)\n",
    "    ks.append(k)\n",
    "\n",
    "# Silhouette Score is only defined for k > 1\n",
    "    score = silhouette_score(X, labels)\n",
    "    silhouette_scores.append(score)\n",
    "    print(f\"k = {k}, Inertia = {inertia:.2f}, Silhouette Score = {score:.3f}\")\n",
    "\n",
    "# Plotting\n",
    "fig, ax1 = plt.subplots(figsize=(9, 5))\n",
    "\n",
    "color1 = 'tab:blue'\n",
    "ax1.set_xlabel('Number of Clusters (k)')\n",
    "ax1.set_ylabel('Inertia (Elbow)', color=color1)\n",
    "ax1.plot(ks, inertias, marker='o', color=color1, label='Inertia')\n",
    "ax1.tick_params(axis='y', labelcolor=color1)\n",
    "\n",
    "ax2 = ax1.twinx()  # second y-axis for Silhouette Score\n",
    "color2 = 'tab:green'\n",
    "ax2.set_ylabel('Silhouette Score', color=color2)\n",
    "ax2.plot(ks, silhouette_scores, marker='s', linestyle='--', color=color2, label='Silhouette Score')\n",
    "ax2.tick_params(axis='y', labelcolor=color2)\n",
    "\n",
    "plt.title('KMeans: Elbow Plot & Silhouette Score')\n",
    "fig.tight_layout()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ebfb3d-e9b9-4251-99d2-75a3d77cdb18",
   "metadata": {},
   "source": [
    "## Code: KMeans Clustering\n",
    "\n",
    "Nutzen Sie die zuvor berechneten Scores, um eine fundierte Entscheidung über die Clusteranzahl zu treffen.  \n",
    "Passen Sie bei Bedarf das Feature-Set oder die Anzahl der PCA-Komponenten an (_springen Sie hierfür zur PCA-Analyse zurück und änderen Sie die entsprechenden Werte. Führen Sie von dort an den Code der Reihe nach erneut bis hier hin aus_) und beobachten Sie, wie sich die Clusterqualität verändert. Im Vergleich zur vorherigen durchgeführten Cluster-Analyse ist nun die Anzahl der Cluster von Ihnen festgelegt und dem original Dataframe wird eine Spalte hinzugefügt, die jedem Indent in eine dieser Cluster einteilt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8de95e-98b2-4088-85bc-f5bfa96a2ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# === change parameter from here ===\n",
    "use_pca = True  # True = using PCA axis for Clustering, False = using orignal feature set \n",
    "n_clusters = 2  # number of clusters\n",
    "#=========================#\n",
    "\n",
    "#=== dont change paramter from here ===#\n",
    "\n",
    "if use_pca:\n",
    "    # extract PCA columns from MultiIndex\n",
    "    X_kmeans = df['PCA'].dropna()\n",
    "else:\n",
    "    # using the feature set defind early as \"features = ...\" \n",
    "    X_kmeans = features.copy()\n",
    "\n",
    "# apllying index columns to the KMeans DataFrame\n",
    "X_kmeans = X_kmeans.copy()\n",
    "X_kmeans.index.name = df.index.name\n",
    "\n",
    "# KMeans calculations \n",
    "kmeans = KMeans(n_clusters=n_clusters, n_init='auto', random_state=42)\n",
    "labels = kmeans.fit_predict(X_kmeans)\n",
    "\n",
    "# using strings for labelings instaed of numbers\n",
    "label_names = [f\"Gruppe {i+1}\" for i in labels]\n",
    "labels_df = pd.DataFrame(label_names, index=X_kmeans.index)\n",
    "\n",
    "# creating MultiIndex colum name for joining labels_df with df\n",
    "labels_df.columns = pd.MultiIndex.from_tuples([('KMeans', 'Label')])\n",
    "\n",
    "# deleting KMeans grouping if it already exists\n",
    "if ('KMeans', 'Label') in df.columns:\n",
    "    df = df.drop(columns=('KMeans', 'Label'))\n",
    "\n",
    "# join KMeans Label_DataFrame with the original Dataframe df\n",
    "df = df.join(labels_df)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc9096b-ddef-4849-bbc6-d99c6c96f214",
   "metadata": {},
   "source": [
    "# Clustering mit Gaussian Mixture Models (GMM)\n",
    "\n",
    "## Was ist GMM?\n",
    "\n",
    "Gaussian Mixture Models (GMM) sind eine probabilistische Erweiterung des KMeans-Algorithmus. Anstatt Datenpunkte strikt einem Cluster zuzuordnen, geht GMM davon aus, dass die Daten aus einer **Mischung mehrerer mehrdimensionaler Normalverteilungen** stammen. Jeder Datenpunkt erhält dabei **Wahrscheinlichkeiten**, mit denen er zu den einzelnen Clustern gehört – das nennt man **weiches Clustering**.\n",
    "\n",
    "Diese Flexibilität erlaubt es GMM, **Cluster mit unterschiedlicher Form, Größe und Orientierung** zu modellieren – im Gegensatz zu KMeans, das nur kugelförmige, gleich große Cluster erkennt.\n",
    "\n",
    "Der Algorithmus nutzt das **Expectation-Maximization (EM)**‑Verfahren und läuft iterativ ab:\n",
    "\n",
    "1. **Initialisierung**  \n",
    "   Es werden `k` Gaußverteilungen mit Startparametern (Mittelwert, Kovarianzmatrix, Mischungsgewicht) gesetzt.\n",
    "\n",
    "2. **E‑Schritt (Expectation)**  \n",
    "   Für jeden Datenpunkt wird berechnet, **mit welcher Wahrscheinlichkeit** er zu jedem Cluster gehört.\n",
    "\n",
    "3. **M‑Schritt (Maximization)**  \n",
    "   Die Parameter der Gaußverteilungen werden aktualisiert – basierend auf den berechneten Wahrscheinlichkeiten.\n",
    "\n",
    "4. **Wiederholung**  \n",
    "   Die Schritte 2 und 3 werden wiederholt, bis sich die Parameter kaum noch ändern oder ein Abbruchkriterium erreicht ist.\n",
    "\n",
    "GMM erlaubt **überlappende Cluster**, unterschiedliche Clusterformen und ist flexibler als KMeans, da es nicht von kugelförmigen Clustern ausgeht.\n",
    "\n",
    "---\n",
    "\n",
    "## Bewertung mit dem Log-Likelihood\n",
    "\n",
    "Da GMM auf Wahrscheinlichkeiten basiert, wird die Modellgüte nicht über Abstände (wie bei KMeans), sondern über den **Log-Likelihood** bewertet. Dieser gibt an, **wie wahrscheinlich es ist, dass das Modell die beobachteten Daten erzeugt hat**.\n",
    "\n",
    "- **Je höher der Log-Likelihood, desto besser passt das Modell zu den Daten.**\n",
    "- Um die Ergebnisse besser mit KMeans vergleichen zu können, wird häufig der **negative Log-Likelihood** geplottet – so entsteht ein ähnlicher „Elbow-Plot“, bei dem man nach einem Knick sucht.\n",
    "\n",
    "---\n",
    "\n",
    "## Wann ist GMM sinnvoll?\n",
    "\n",
    "GMM ist besonders hilfreich, wenn:\n",
    "\n",
    "- **Cluster unterschiedliche Formen oder Dichten aufweisen**\n",
    "- **Cluster sich überlappen** und eine harte Zuweisung (wie bei KMeans) zu ungenau wäre\n",
    "- **Wahrscheinlichkeiten für die Clusterzugehörigkeit** von Interesse sind (z. B. zur Identifikation unsicherer oder gemischter Bereiche)\n",
    "- **Materialphasen oder Gefügezustände fließend ineinander übergehen**, statt klar getrennt zu sein\n",
    "\n",
    "---\n",
    "\n",
    "## Code: Bestimmung der geeigneten Anzahl an Cluster\n",
    "Füren Sie den unten stehenden Code-Abschnitt aus und analysieren sie welche Cluster-Anzahl für für den GMM-Algorithmus auf Grund des zuvor definierten Feature-Sets und der PCA Analyse am sinnvollsten ist. Passen sie gegebenenfalls die PCA-Analyse an. Gehen Sie wie beim KMeans-Algorithmus vor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6655a37-c25d-4222-bb41-05e02571997f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GMM Clustering\n",
    "import os \n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "\"\"\"\n",
    "Computes and plots Silhouette Score and Negative Log-Likelihood (as Elbow Plot) \n",
    "for different numbers of clusters using Gaussian Mixture Models (GMM).\n",
    "\n",
    "Parameters:\n",
    "- df: DataFrame containing the data and possibly PCA columns\n",
    "- features_source: 'PCA' or 'features' → determines which data basis to use\n",
    "- max_number_cluster: maximum number of clusters to test (e.g., 10)\n",
    "\"\"\"\n",
    "\n",
    "# === Change from here === #\n",
    "features_source = \"PCA\"  # <-- \"PCA\" or \"features\"\n",
    "max_number_cluster = 10\n",
    "# ======================== #\n",
    "\n",
    "# === Don't change from here === #\n",
    "silhouette_scores = []\n",
    "neg_log_likelihoods = []\n",
    "ks = []\n",
    "\n",
    "# Select data basis\n",
    "if features_source == 'PCA':\n",
    "    X = df['PCA'].dropna()\n",
    "elif features_source == 'features':\n",
    "    X = features.copy()\n",
    "else:\n",
    "    raise ValueError(\"features_source must be 'PCA' or 'features'.\")\n",
    "\n",
    "for k in range(2, max_number_cluster):\n",
    "    gmm = GaussianMixture(n_components=k, random_state=42, n_init=5)\n",
    "    gmm.fit(X)\n",
    "    labels = gmm.predict(X)\n",
    "    \n",
    "    # Log-likelihood (higher is better, so we invert it for Elbow-like plot)\n",
    "    neg_ll = -gmm.score(X) * len(X)\n",
    "    neg_log_likelihoods.append(neg_ll)\n",
    "    ks.append(k)\n",
    "\n",
    "    # Silhouette Score\n",
    "    score = silhouette_score(X, labels)\n",
    "    silhouette_scores.append(score)\n",
    "    print(f\"k = {k}, -LogLikelihood = {neg_ll:.2f}, Silhouette Score = {score:.3f}\")\n",
    "\n",
    "# Plotting\n",
    "fig, ax1 = plt.subplots(figsize=(9, 5))\n",
    "\n",
    "color1 = 'tab:purple'\n",
    "ax1.set_xlabel('Number of Clusters (k)')\n",
    "ax1.set_ylabel('− Log-Likelihood (Elbow)', color=color1)\n",
    "ax1.plot(ks, neg_log_likelihoods, marker='o', color=color1, label='− Log-Likelihood')\n",
    "ax1.tick_params(axis='y', labelcolor=color1)\n",
    "\n",
    "ax2 = ax1.twinx()  # second y-axis for Silhouette Score\n",
    "color2 = 'tab:green'\n",
    "ax2.set_ylabel('Silhouette Score', color=color2)\n",
    "ax2.plot(ks, silhouette_scores, marker='s', linestyle='--', color=color2, label='Silhouette Score')\n",
    "ax2.tick_params(axis='y', labelcolor=color2)\n",
    "\n",
    "plt.title('GMM: Log-Likelihood & Silhouette Score')\n",
    "fig.tight_layout()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b641840-cc79-40f1-9b12-d61e4bc32427",
   "metadata": {},
   "source": [
    "## Code: GMM Clustering\n",
    "\n",
    "Nutzen Sie die zuvor berechneten Scores (z. B. BIC, AIC oder Silhouette), um eine fundierte Entscheidung über die optimale Anzahl an Clustern für das GMM zu treffen.\n",
    "\n",
    "Falls notwendig:\n",
    "\n",
    "- passen Sie das **Feature‑Set** an oder  \n",
    "- verändern Sie die Anzahl der **PCA‑Komponenten**\n",
    "\n",
    "(gehen Sie hierfür zur PCA‑Analyse zurück, ändern Sie die entsprechenden Werte und führen Sie den Code erneut bis zu diesem Punkt aus).\n",
    "\n",
    "Beobachten Sie, wie sich die Clusterqualität verändert.\n",
    "\n",
    "Im Gegensatz zur vorherigen Clusteranalyse legen Sie nun die Anzahl der Cluster selbst fest.  \n",
    "Dem ursprünglichen DataFrame wird anschließend eine neue Spalte hinzugefügt, die jedem Datenpunkt ein GMM‑Cluster zuweist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4667dad0-62fa-4407-b704-b7bd6c540f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# === change parameter from here ===\n",
    "use_pca = True   # True = using PCA axis for Clustering, False = using original feature set \n",
    "n_clusters = 2   # number of mixture components\n",
    "gmm_n_init = 10   # number of initializations for GMM\n",
    "# ======================== #\n",
    "\n",
    "# === don't change parameter from here === #\n",
    "\n",
    "if use_pca:\n",
    "    # extract PCA columns from MultiIndex\n",
    "    X_gmm = df['PCA'].dropna()\n",
    "else:\n",
    "    # using the feature set defined earlier as \"features = ...\"\n",
    "    X_gmm = features.copy()\n",
    "\n",
    "# apply index columns to the GMM DataFrame\n",
    "X_gmm = X_gmm.copy()\n",
    "X_gmm.index.name = df.index.name\n",
    "\n",
    "# GMM fitting\n",
    "gmm = GaussianMixture(n_components=n_clusters, random_state=42, n_init=gmm_n_init)\n",
    "gmm.fit(X_gmm)\n",
    "labels = gmm.predict(X_gmm)\n",
    "probs = gmm.predict_proba(X_gmm)  # shape: (n_samples, n_components)\n",
    "\n",
    "# using strings for labelings instead of numbers\n",
    "label_names = [f\"Gruppe {i+1}\" for i in labels]\n",
    "labels_df = pd.DataFrame(label_names, index=X_gmm.index)\n",
    "\n",
    "# create MultiIndex column name for joining labels_df with df\n",
    "labels_df.columns = pd.MultiIndex.from_tuples([('GMM', 'Label')])\n",
    "\n",
    "# create a DataFrame for component probabilities with MultiIndex columns\n",
    "prob_cols = [ ( 'GMM', f'comp_{i+1}' ) for i in range(n_clusters) ]\n",
    "prob_df = pd.DataFrame(probs, index=X_gmm.index, columns=pd.MultiIndex.from_tuples(prob_cols))\n",
    "\n",
    "# delete existing GMM grouping/prob columns if they already exist\n",
    "cols_to_drop = []\n",
    "if ('GMM', 'Label') in df.columns:\n",
    "    cols_to_drop.append(('GMM', 'Label'))\n",
    "for col in prob_df.columns:\n",
    "    if col in df.columns:\n",
    "        cols_to_drop.append(col)\n",
    "if cols_to_drop:\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "# join label and probability DataFrames with the original DataFrame df\n",
    "df = df.join(labels_df)\n",
    "df = df.join(prob_df)\n",
    "\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb37b9f-4017-4fb3-b5a0-9a2e45ba919a",
   "metadata": {},
   "source": [
    "# Cluster Mapping\n",
    "## Code: Plotting der Clusterlabels auf die x‑ und y‑Koordinaten\n",
    "\n",
    "Verwenden Sie den untenstehenden Code, um Cluster‑Mappings zu erstellen.  \n",
    "Im Gegensatz zum Beginn der Analyse können nun nicht mehr nur einzelne mechanische Werte wie E‑Modul oder Härte visualisiert werden, sondern auch die **Clusterlabels** selbst.\n",
    "\n",
    "Dadurch entsteht eine Karte, die Bereiche mit ähnlichen Eigenschaften farblich hervorhebt und so räumliche Muster oder Materialzonen sichtbar macht.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d13e8e-6a83-4322-8d60-e7ffae21a00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=== change parameter from here ===#\n",
    "#marker options\n",
    "marker_shape = \"s\" #<-- \"s\" for squares, \">\" for triangles\n",
    "marker_size = 85 # setting the size of the markers\n",
    "marker_tranparency = 0.75 # setting the transparency of the markers\n",
    "\n",
    "#choosing between ideal indent position and real indent positions\n",
    "indent_position = \"real\" # <-- \"real\" or \"ideal\"\n",
    "\n",
    "#change the colormap of the plots\n",
    "colormap = \"tab10\" #<-- \"tab10\", \"tab20\", \"colorblind\" \n",
    "\n",
    "hue = (\"GMM\",\"Label\")\n",
    "#=========================#\n",
    "\n",
    "#=== dont change paramter from here ===#\n",
    "#image size\n",
    "image_width_cm = 6 #change value to alter the image size\n",
    "image_height_cm = 6 # change value to alter the image size\n",
    "\n",
    "#Atomic Force Microscopy Mapping of Indentationmapping\n",
    "background_image = plt.imread(\"Data/Images/BackGround.png\")\n",
    "dx,dy = -2.5,-3.2 #parameter for adjusting the background image\n",
    "range_um = 50 # parameter for adjusting the background image\n",
    "\n",
    "# automatical column selection for indent position\n",
    "if indent_position == \"real\":\n",
    "    x = (\"x\",\"real\") # defining x axis\n",
    "    y = (\"y\",\"real\") # defining y axis\n",
    "elif indent_position ==\"ideal\":\n",
    "    x = (\"x\",\"absolut\") # defining x axis\n",
    "    y = (\"y\",\"absolut\") # defining x axis\n",
    "\n",
    "#creating a figure object\n",
    "fig, ax = plt.subplots(nrows = 1,\n",
    "                       ncols = 1, \n",
    "                       sharey=False)\n",
    "\n",
    "\n",
    "fig.set_dpi(600) # increasing the resolution of the plot\n",
    "fig.set_size_inches(image_width_cm/2.5,image_height_cm/2.54) #calcuating image size\n",
    "\n",
    "\n",
    "sns.scatterplot(data = df,\n",
    "                x = x,\n",
    "                y = y,\n",
    "                hue = hue,\n",
    "                ax = ax,\n",
    "                marker = marker_shape, #square marker\n",
    "                palette = sns.color_palette(colormap)[2:], # defining the color plaette\n",
    "                s = marker_size, # marker size\n",
    "                edgecolor = None, # remove the outline of the markers\n",
    "                alpha = marker_tranparency , # transparence of the markers infill\n",
    "               )\n",
    "\n",
    "if indent_position == \"real\":\n",
    "    ax.imshow(background_image,\n",
    "                 extent=[dx,range_um+dx,dy,range_um+dy],\n",
    "                 aspect='equal',\n",
    "                 zorder=-1,\n",
    "                 origin = \"upper\"\n",
    "                )\n",
    "\n",
    "ax.set_aspect(\"equal\") # ensure äquidistance on x and y axis\n",
    "sns.move_legend(loc = \"lower center\", bbox_to_anchor = (0.5,1), obj = ax) # move legend above the corresponding plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f17e3a-2777-4a3d-97b3-038a38193e0d",
   "metadata": {},
   "source": [
    "# Datenaufbereitung\n",
    "\n",
    "Betrachtet man den Silhouette Score und den Elbow-Plot für die GMM‑ und KMeans‑Analyse, fällt auf, dass für die meisten Feature-Sets oder PCA‑Achsen weder bei GMM noch bei KMeans ein klar erkennbarer Knick im Elbow-Plot vorhanden ist. Zudem liegt der Silhouette Score in den meisten Fällen unterhalb von 0.4. Dies weist darauf hin, dass die Trennschärfe zwischen den einzelnen Clustern sehr gering ist.\n",
    "\n",
    "Ursachen hierfür können sein:\n",
    "\n",
    "- ein zu kleiner Datensatz (zu wenige Indents)  \n",
    "- eine zu große Streuung innerhalb der Daten  \n",
    "\n",
    "Welche Variante — mehr Daten mit höherer Streuung oder weniger Daten mit geringerer Streuung — für einen Datensatz vorteilhafter ist, muss empirisch untersucht werden.\n",
    "\n",
    "Für diese Bewertung ist ein gutes Verständnis der vorliegenden Daten entscheidend:\n",
    "\n",
    "- Wie wurden die Daten erhoben?  \n",
    "- Welche physikalische Bedeutung besitzen sie?\n",
    "\n",
    "Laden Sie die Datei **„DataWithAdditional_Infos.csv“** und verschaffen Sie sich einen Überblick über den Datensatz.  \n",
    "Alle Mittelwerte („mean“) und Standardabweichungen („std“) wurden über einen Eindringtiefenbereich von **200 bis 400 nm** berechnet. Die zugehörigen Kraft‑Weg‑, E‑Modul‑Weg‑ und Härte‑Weg‑Datensätze sind aufgrund der Datenmenge nicht Teil dieser Übung.\n",
    "\n",
    "Über denselben Eindringtiefenbereich (200–400 nm) wurde ein linearer Fit für die Härte‑Weg‑, E‑Modul‑Weg‑ und Steifigkeit‑Weg‑Datensätze durchgeführt. Dieser Fit ermöglicht es zu beurteilen, ob die Werte (E‑Modul, Härte) über die betrachtete Eindringtiefe steigen, fallen oder konstant bleiben.  \n",
    "Ab einer ausreichend großen Eindringtiefe sollten die Werte weitgehend konstant sein. Ist dies nicht der Fall, kann dies darauf hindeuten, dass:\n",
    "\n",
    "- ein Indent zwei Phasen gleichzeitig getroffen hat oder  \n",
    "- sich im Bereich 0–400 nm mehrere Phasen untereinander befinden.\n",
    "\n",
    "Solche Indents verwischen die Clustergrenzen und können die Clusterqualität deutlich verschlechtern. Daher kann es sinnvoll sein, diese Indents aus dem Datensatz zu entfernen.\n",
    "\n",
    "---\n",
    "\n",
    "## Code: Datenbearbeitung\n",
    "\n",
    "Führen Sie mithilfe des untenstehenden Codes eine Datenfilterung durch und entfernen Sie alle Indents, deren E‑Modul im Auswertebereich der Eindringtiefe (200–400 nm) um mehr als **10 %** schwankt.  \n",
    "Die entsprechende Spalte im Datensatz lautet: **(\"MODULUS GPa\", \"Slope%\")**.\n",
    "\n",
    "Lassen Sie den Code bei Bedarf mehrfach laufen und verändern Sie jeweils:\n",
    "\n",
    "- den Parameter `value_to_filter`  \n",
    "- die Werte für `min_value` und `max_value`\n",
    "\n",
    "um nach mehreren Kriterien zu filtern.\n",
    "\n",
    "Führen Sie anschließend alle Codeabschnitte ab der **Principal Component Analysis (PCA)** erneut aus und überprüfen Sie, ob sich der Elbow‑Plot oder der Silhouette Score für KMeans und GMM verbessert hat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef84eb26-ac49-4428-8fda-c81a34c4ba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_to_filter = (\"MODULUS GPa\",\"Slope%\")\n",
    "min_value = -5\n",
    "max_value = 5\n",
    "\n",
    "df = df[(df[value_to_filter] <= max_value) &(df[value_to_filter] >= min_value)]\n",
    "\n",
    "#only to show that the length of the DataFrame has changed\n",
    "display(df[(\"MODULUS GPa\",\"Slope%\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7c319a-03c3-4609-9bfd-e7ef241d7541",
   "metadata": {},
   "source": [
    "# Weitere Cluster‑Darstellungen\n",
    "\n",
    "## Code: Plotten unterschiedlicher Cluster‑Darstellungen\n",
    "\n",
    "Verwenden Sie den untenstehenden Code, um verschiedene Darstellungen der Cluster im Feature‑Space oder im PCA‑Space zu visualisieren.  \n",
    "Wählen Sie hierfür für **x**, **y** und **z** die jeweiligen Spalten aus, die gegeneinander aufgetragen werden sollen.  \n",
    "Über den Parameter **hue** legen Sie fest, nach welchem Kriterium die Datenpunkte eingefärbt werden.\n",
    "\n",
    "Besonders geeignet sind die Clusterlabels  \n",
    "- **(\"GMM\", \"Label\")** oder  \n",
    "- **(\"KMeans\", \"Label\")**,  \n",
    "\n",
    "aber auch kontinuierliche (nicht‑kategorische) Spalten können verwendet werden, um zusätzliche datenabhängige Zusammenhänge sichtbar zu machen.\n",
    "\n",
    "Für eine **2D‑Darstellung** setzen Sie den Parameter:\n",
    "z = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61684060-7ebc-4c1b-9efa-5615f34e1451",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#=== change from here ===#\n",
    "plot_dataframe(df, #<-- keep it \n",
    "               x=('MODULUS GPa','mean'), # choose x-axis\n",
    "               y=(\"HARDNESS GPa\",'mean'), # choose y-axis\n",
    "               z = (\"S2overP\",\"mean\"), # choose z-axis or set it to None for 2D plot\n",
    "               hue=('KMeans','Label') # choosing colorations \n",
    "              ) \n",
    "#=== dont change from here ===#\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
